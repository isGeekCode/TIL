#!/usr/bin/env python3
"""
README.mdì˜ ë§í¬ë¥¼ ë¶„ì„í•˜ì—¬ í†µê³„ë¥¼ ì¶œë ¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
import re
import os
from pathlib import Path
from collections import defaultdict

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = Path(__file__).parent
README_PATH = SCRIPT_DIR / "README.md"

def get_file_content_size(file_path):
    """íŒŒì¼ì˜ ë‚´ìš© í¬ê¸°ë¥¼ í™•ì¸ (ë°”ì´íŠ¸)"""
    try:
        if not file_path.exists():
            return -1  # íŒŒì¼ ì—†ìŒ

        content = file_path.read_text(encoding='utf-8')
        # ê³µë°±, ê°œí–‰ ë“±ì„ ì œì™¸í•œ ì‹¤ì œ ë‚´ìš©ë§Œ ê³„ì‚°
        stripped_content = content.strip()

        return len(stripped_content)
    except Exception as e:
        return -2  # ì—ëŸ¬

def analyze_markdown_links(readme_content):
    """READMEì˜ ë§ˆí¬ë‹¤ìš´ ë§í¬ë¥¼ ë¶„ì„"""

    # ë§ˆí¬ë‹¤ìš´ ë§í¬ íŒ¨í„´: [í…ìŠ¤íŠ¸](ë§í¬)
    link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'

    stats = {
        'total_links': 0,
        'external_links': 0,
        'anchor_links': 0,
        'file_links': 0,
        'files_with_content': 0,
        'empty_files': 0,
        'missing_files': 0,
        'error_files': 0,
    }

    files_by_category = {
        'with_content': [],
        'empty': [],
        'missing': [],
        'error': []
    }

    for match in re.finditer(link_pattern, readme_content):
        link_text = match.group(1)
        link_url = match.group(2)

        stats['total_links'] += 1

        # ì™¸ë¶€ ë§í¬
        if link_url.startswith('http'):
            stats['external_links'] += 1
            continue

        # ì•µì»¤ ë§í¬
        if link_url.startswith('#'):
            stats['anchor_links'] += 1
            continue

        # íŒŒì¼ ë§í¬
        stats['file_links'] += 1

        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        file_path = SCRIPT_DIR / link_url

        # íŒŒì¼ ë‚´ìš© í™•ì¸
        content_size = get_file_content_size(file_path)

        if content_size == -1:
            stats['missing_files'] += 1
            files_by_category['missing'].append((link_text, link_url))
        elif content_size == -2:
            stats['error_files'] += 1
            files_by_category['error'].append((link_text, link_url))
        elif content_size < 100:  # 100ì ë¯¸ë§Œì€ ë¹ˆ íŒŒì¼ë¡œ ê°„ì£¼
            stats['empty_files'] += 1
            files_by_category['empty'].append((link_text, link_url, content_size))
        else:
            stats['files_with_content'] += 1
            files_by_category['with_content'].append((link_text, link_url, content_size))

    return stats, files_by_category

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print(f"ğŸ“Š README.md ë§í¬ ë¶„ì„ ì‹œì‘\n")
    print(f"README ê²½ë¡œ: {README_PATH}\n")

    # README íŒŒì¼ ì½ê¸°
    if not README_PATH.exists():
        print(f"âŒ ERROR: README.mdë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {README_PATH}")
        return

    readme_content = README_PATH.read_text(encoding='utf-8')

    # ë§í¬ ë¶„ì„
    stats, files_by_category = analyze_markdown_links(readme_content)

    # í†µê³„ ì¶œë ¥
    print("=" * 60)
    print("ğŸ“ˆ ì „ì²´ í†µê³„")
    print("=" * 60)
    print(f"ì „ì²´ ë§í¬ ìˆ˜: {stats['total_links']}")
    print(f"  - ì™¸ë¶€ ë§í¬ (http): {stats['external_links']}")
    print(f"  - ì•µì»¤ ë§í¬ (#): {stats['anchor_links']}")
    print(f"  - íŒŒì¼ ë§í¬: {stats['file_links']}")
    print()
    print(f"íŒŒì¼ ë§í¬ ìƒì„¸:")
    print(f"  âœ… ë‚´ìš© ìˆëŠ” íŒŒì¼: {stats['files_with_content']} ({stats['files_with_content']/stats['file_links']*100:.1f}%)")
    print(f"  ğŸ“ ë¹ˆ íŒŒì¼ (100ì ë¯¸ë§Œ): {stats['empty_files']} ({stats['empty_files']/stats['file_links']*100:.1f}%)")
    print(f"  âŒ íŒŒì¼ ì—†ìŒ: {stats['missing_files']} ({stats['missing_files']/stats['file_links']*100:.1f}%)")
    if stats['error_files'] > 0:
        print(f"  âš ï¸  ì½ê¸° ì—ëŸ¬: {stats['error_files']}")
    print()

    # ë¹ˆ íŒŒì¼ ëª©ë¡ ì¶œë ¥ (ì²˜ìŒ 10ê°œë§Œ)
    if files_by_category['empty']:
        print("=" * 60)
        print(f"ğŸ“ ë¹ˆ íŒŒì¼ ëª©ë¡ (ì²˜ìŒ 10ê°œ)")
        print("=" * 60)
        for i, (text, url, size) in enumerate(files_by_category['empty'][:10], 1):
            print(f"{i:2d}. [{text}]({url}) - {size}ì")
        if len(files_by_category['empty']) > 10:
            print(f"    ... ì™¸ {len(files_by_category['empty']) - 10}ê°œ")
        print()

    # íŒŒì¼ ì—†ìŒ ëª©ë¡ ì¶œë ¥ (ì²˜ìŒ 10ê°œë§Œ)
    if files_by_category['missing']:
        print("=" * 60)
        print(f"âŒ íŒŒì¼ ì—†ìŒ ëª©ë¡ (ì²˜ìŒ 10ê°œ)")
        print("=" * 60)
        for i, (text, url) in enumerate(files_by_category['missing'][:10], 1):
            print(f"{i:2d}. [{text}]({url})")
        if len(files_by_category['missing']) > 10:
            print(f"    ... ì™¸ {len(files_by_category['missing']) - 10}ê°œ")
        print()

    # ê²°ë¡ 
    print("=" * 60)
    print("ğŸ’¡ ê²°ë¡ ")
    print("=" * 60)
    total_problematic = stats['empty_files'] + stats['missing_files']
    problematic_ratio = total_problematic / stats['file_links'] * 100 if stats['file_links'] > 0 else 0

    if problematic_ratio < 10:
        print(f"âœ… ë¬¸ì œ ë¹„ìœ¨: {problematic_ratio:.1f}% - ë‚®ì€ í¸ì…ë‹ˆë‹¤.")
        print("   â†’ ë§í¬ ì œê±° ì‘ì—…ì„ ì§„í–‰í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.")
    elif problematic_ratio < 30:
        print(f"âš ï¸  ë¬¸ì œ ë¹„ìœ¨: {problematic_ratio:.1f}% - ë³´í†µì…ë‹ˆë‹¤.")
        print("   â†’ ë§í¬ ì œê±° ë˜ëŠ” ì„¹ì…˜ ë¶„ë¦¬ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
    else:
        print(f"âŒ ë¬¸ì œ ë¹„ìœ¨: {problematic_ratio:.1f}% - ë†’ì€ í¸ì…ë‹ˆë‹¤.")
        print("   â†’ ì„¹ì…˜ì„ ë¶„ë¦¬í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        print("   â†’ 'âœ… ì‘ì„± ì™„ë£Œ' / 'ğŸ“ ì‘ì„± ì˜ˆì •' ì„¹ì…˜ìœ¼ë¡œ ë‚˜ëˆ„ê¸°")

if __name__ == "__main__":
    main()
